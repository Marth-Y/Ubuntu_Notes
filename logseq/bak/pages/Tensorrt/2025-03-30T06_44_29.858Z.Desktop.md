- >[CUDA C++ Best Practices Guide](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html)
  [Developer Guide :: NVIDIA Deep Learning TensorRT Documentation](https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html)
  [Microsoft Word - CUDA PG 1 1 chs.doc](https://www.nvidia.cn/docs/IO/51635/NVIDIA_CUDA_Programming_Guide_1.1_chs.pdf)
  这个看起来比较基础 [CUDA C++ Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html)
- # CPU与GPU对比
  collapsed:: true
	- ## 关键字
		- `ltency` #card
		  card-last-score:: 1
		  card-repeats:: 1
		  card-next-schedule:: 2024-12-05T16:00:00.000Z
		  card-last-interval:: -1
		  card-ease-factor:: 2.36
		  card-last-reviewed:: 2024-12-05T09:40:06.410Z
			- 完成一个指令所需要的时间
		- `memory latency` #card
		  card-last-interval:: -1
		  card-repeats:: 1
		  card-ease-factor:: 2.36
		  card-next-schedule:: 2024-12-05T16:00:00.000Z
		  card-last-reviewed:: 2024-12-05T09:40:19.727Z
		  card-last-score:: 1
			- CPU/GPU从memory获取数据所需要的等待时间。
			- CPU并行处理的优化的主要方向
		- `throughput` #card
		  card-last-interval:: 4
		  card-repeats:: 2
		  card-ease-factor:: 2.46
		  card-next-schedule:: 2024-12-09T09:40:23.834Z
		  card-last-reviewed:: 2024-12-05T09:40:23.835Z
		  card-last-score:: 5
			- 吞吐量，单位时间内可以执行的指令数
			- GPU并行处理的优化的主要方向
		- `Multi-threading` #card
		  card-last-interval:: 4
		  card-repeats:: 2
		  card-ease-factor:: 2.7
		  card-next-schedule:: 2024-12-09T09:41:21.683Z
		  card-last-reviewed:: 2024-12-05T09:41:21.684Z
		  card-last-score:: 5
			- 多线程
	- ## CPU与GPU在并行处理的优化方向
		- CPU主要目标在于减少memory latency
			- 因为CPU程序一般命令比较复杂，并行化的程度不会很高，加再多和core后续也无法提高运行速率，而主要卡点在于memory latency
		- GPU主要目标在于提高throughput
			- GPU主要用于图像处理，大量的计算，这些计算都是独立的，天生适合并行处理。
	- ## CPU如何优化？
		- 提高throughput
			- Pipeline
				- CPU instruction pipeline
					- 一条指令可以分为取指令等一系列操作，就可以将其流水线化
					- ![1730986745889.png](../assets/1730986745889_1730986753246_0.png)
			- multi-thread
				- 提高throughput的一种优化
				- 多线程
				- 让因为数据IO或者cache miss而stall的core去做其他事情
		- 减少memory latency
			- cache hierarchy
				- 多级缓存，L1 L2 L3 cache
			- pre-fetch
				- 提前取数据等
				- 将需要的数据、指令等一次性读出
			- branch-prediction
				- 分支预测(if-else等条件语句)。根据以往的branch走向，去预测下一次branch的走向，从而pre-fetch等，预测失败就rollback就可以
				- CPU有专门的硬件负责预测
	- ## GPU的特点
		- SIMT
			- 类似于SIMD的一种概念，将一条指令分给大量的thread去执行，thread间的调度是由warp(GPU体系架构中有一个warp schedular专门负责管理线程调度)来负责管理
			- [[SIMD]]
	- ## CPU与GPU的异同：
		- CPU：
			- 适合复杂的逻辑运算——因此过大的增大core的数量并不能很好的提高throughput
			- 优化方向在于减少memory latency
			- 不同于GPU，硬件上有专门分支预测器去实现 branch-prediction
		- GPU
			- 适合简单单一的大规模运算。如图像处理、深度学习等
			- 优化方向在于提高throughput
			- 不同于CPU，GPU硬件上有wrap schedular去进行多线程的调度
			- 由于GPU经常处理大规模运算，会提前把数据准备好，所以在throughtput很高的情况下，GPU内部的memory latency上带来的性能损失不明显，但是CPU和GPU之间通信时产生的memory latency需要重视
- # tensorrt环境搭建
  collapsed:: true
	- > 理解cuda,cudnn,tensorrt的版本选择，以及如何使用docker
	- tensorrt依赖cuda和cudnn，所以看见tensorrt版本后可以去`tensorrt release note`去找到对应版本，以及支持的cuda 和 cudnn版本
	- 在官网下载run脚本安装cuda
	  logseq.order-list-type:: number
		- 多版本管理：将`/usr/local/cuda`软链接指向不同的cuda版本，然后修改环境变量`PATH`、`LD_LIBRARY_PATH`即可
		  logseq.order-list-type:: number
		- 版本查看`nvcc -V`
		  logseq.order-list-type:: number
	- 安装cudnn
	  logseq.order-list-type:: number
		- ```bash
		  Navigate to your <cudnnpath> directory containing the cuDNN tar file.
		  Unzip the cuDNN package.
		  $ tar -xvf cudnn-linux-x86_64-8.x.x.x_cudaX.Y-archive.tar.xz
		  Copy the following files into the CUDA toolkit directory.
		  $ sudo cp cudnn-*-archive/include/cudnn*.h /usr/local/cuda/include 
		  $ sudo cp -P cudnn-*-archive/lib/libcudnn* /usr/local/cuda/lib64 
		  $ sudo chmod a+r /usr/local/cuda/include/cudnn*.h /usr/local/cuda/lib64/libcudnn*
		  ```
		- 查看cudnn版本：在cudnn的头文件中有一个`cudnn_version.h`其中记录了版本信息
	- 安装tensorrt
	  logseq.order-list-type:: number
		- 解压下载的压缩包即可有`bin`，`include`和`lib`文件夹了，可以修改环境变量`PATH`，`LD_LIBRARY_PATH`。同理可以用环境变量来进行版本控制
		- 验证安装成功：`trtexec`，并且在末尾会显示版本信息
		- 可以进入sample文件夹中学习一些API示例，或者make运行一下
- # NVIDIA Docker #docker
  collapsed:: true
	- 根据官方文档安装NVIDIA Container toolkit([Installing the NVIDIA Container Toolkit — NVIDIA Container Toolkit 1.16.2 documentation](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#configuration))
	  logseq.order-list-type:: number
		- `dpkg -l | grep nvidia`可以查看相关包，有`nvidia-docker2`、`nvidia-container-toolkit`、`nvidia-container-toolkit-base`就安装成功了
		  logseq.order-list-type:: number
		- 运行
		  logseq.order-list-type:: number
			- ```bash
			  sudo docker   run       --rm      --runtime=nvidia   --gpus all     ubuntu nvidia-smi
			             运行镜像   运行完后就删除                  使用主机所有gpu        
			  ```
	- 寻找不同的镜像版本：NGC
	  logseq.order-list-type:: number
		- [TensorRT | NVIDIA NGC](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tensorrt/tags)里面会有不同版本的镜像，但是看不到镜像里面的具体内容信息
		- 可以在[Container Release Notes :: NVIDIA Deep Learning TensorRT Documentation](https://docs.nvidia.com/deeplearning/tensorrt/container-release-notes/index.html)寻找对应的tag查看对应的内容信息
		- 然后寻找一个同时满足自己主机端驱动支持的cuda版本以及需要的版本的镜像
	- 创建`dockerfile`
	  logseq.order-list-type:: number
- # cuda basic #cuda
  collapsed:: true
	- ## grid block thrad逻辑结构（内存模型）
	  collapsed:: true
		- ![image.png](../assets/image_1731078953859_0.png){:height 655, :width 1188}
		- 逻辑结构如左图所示(并不对应具体的硬件设计)，存储结构如右图所示
		- 左图解释：
			- 一个kernel代表一个核函数，一个核函数包含一个grid，一个grid包含多个block(一维~三维)，一个block包含多个thread(一维~三维)。
		- 右图解释：
			- 一个grid的所有thread共享三个内存，每个thread有独立的register 和 local memory，同一个block中的thread有一个共享的shared memory
		- 深蓝：
			- 每个线程处理器（SP）都用自己的registers（寄存器）。对应上右图的thread
			- 每个SP都有自己的local memory（局部内存），register和local memory只能被线程自己访问
			- 每个多核处理器（SM）内都有自己的shared memory（共享内存），shared memory 可以被线程块内所有线程访问。相当于block
			- 一个GPU的所有SM共有一块global memory（全局内存），不同线程块的线程都可使用
		- ### CUDA内存读写速度
			- 每个线程读写速度对比：
				- 各自线程寄存器(~1周期)
				- 线程块共享内存(~5周期)
				- Grid全局内存(~500周期)
				- Grid常量内存(~5周期)
	- ## thread遍历，thread index计算
	  collapsed:: true
		- 整体思路都是按照z->y->x三个轴依次遍历。
		- 三个轴的朝向如下所示
		- ### block 中 thread 的遍历
			- ![image.png](../assets/image_1731079740779_0.png)
			- ```cpp
			  __global__ void print_threadidx_in_block() {
			      int index = threadIdx.z * blockDim.x * blockDim.y + 
			                  threadIdx.y * blockDim.x + threadIdx.x;
			      printf("blockDim.x = %d, blockDim.y = %d, blockDim.z = %d, index = %d\n",
			              blockDim.x, blockDim.y, blockDim.z, index);
			  }
			  ```
		- ### grid 中 thread 的遍历
			- 思路：先计算对应于哪一个block，再用上述方式计算block中的索引，最后加上前面block的偏移
			- ![image.png](../assets/image_1731079897277_0.png)
			- ```cpp
			  __global__ void print_threadidx_in_grid() {
			      int block_size = blockDim.x * blockDim.y * blockDim.z;
			  
			      int block_index = blockIdx.z * gridDim.x * gridDim.y + 
			                        blockIdx.y * gridDim.x + blockIdx.x;
			      int thread_index = threadIdx.z * blockDim.x * blockDim.y + 
			                         threadIdx.y * blockDim.x + threadIdx.x;
			      int in_grid_index = block_index * block_size + thread_index;
			      printf("blockDim.x = %d, blockDim.y = %d, blockDim.z = %d, block index = %d\n",
			              blockDim.x, blockDim.y, blockDim.z, block_index);
			      printf("gridDim.x = %d, gridDim.y = %d, gridDim.z = %d, thread index = %d, in_grid_index = %d\n",
			              gridDim.x, gridDim.y, gridDim.z, thread_index, in_grid_index);
			  }
			  ```
			- 在图像中一般寻找的是坐标，因此更推荐以下方法：
				- ![image.png](../assets/image_1731080046215_0.png)
				- 图中红点的x坐标就是`blockIdx.x * blockDim.x + threadIdx.x`，y坐标就是`blockDim.y * blockIdx.y + threadIdx.y`
				-
	- ## 线程束(wrap)
	  collapsed:: true
		- SM采用的SIMT(Single-Instruction, Multiple-Thread，单指令多线程)架构，warp(线程束)是最基本的执行单元，一个warp包含32个并行thread，这些thread以不同数据资源执行相同的指令。warp本质上是线程在GPU上运行的最小单元。
			- 单指令多线程，大白话就是一个线程束的线程执行的是同一条指令，只不过使用的数据不一样
		- 当一个kernel被执行时，grid中的线程块被分配到SM上，一个线程块的thread只能在一个SM上调度，SM一般可以调度多个线程块，大量的thread可能被分到不同的SM上。每个thread拥有它自己的程序计数器和状态寄存器，并且用该线程自己的数据执行指令，这就是所谓的Single Instruction Multiple Thread(SIMT)。
		- 由于warp的大小为32，所以block所含的thread的大小一般要设置为32的倍数。
	- ## 硬件概念
	  collapsed:: true
		- ### SP
			- cuda core也就是stream processor(SP)，是GPU最基本的处理单元。具体指令和任务都是在SP上处理的，GPU并行计算也就是很多SP同时处理，一个SP可以执行一个thread，但是实际上并不是所有的thread能够在同一时刻执行。这里我理解应该是以wrap为单位的并行。
		- ### SM
			- stream multiprocessor。SM包含SP和一起其他资源，一个SM可以包含多个SP。SM可以看作GPU的核心，GPU中每个SM都设计成支持数以百计的线程并行执行，并且每个GPU都包含了很多的SM，所以GPU支持成百上千的线程并行执行。大白话来理解就是SM相当于一个CPU包含了成百计的核，支持成百计的线程并行。
			- ==一个SM在一个时刻只能处理一个block==
	- ## CUDA中的内存模型分为以下几个层次：
	  collapsed:: true
		- 线程处理器（SP）对应线程（thread）
		- 多核处理器（SM）对应线程块（thread block）
		- 设备端（device）对应线程块组合体（grid）
	- ## CPU与GPU同步的几种函数
	  id:: 672e2f6e-602d-4a35-aac4-7edf896c0af8
	  collapsed:: true
		- `cudaDeviceSynchronize();`
			- CPU与GPU端完成同步，CPU不执行之后的语句，直到这个语句以前的所有cuda操作结束
		- `cudaStreamSynchronize(streamid);`
			- >"Blocks host until all CUDA calls in streamid are complete"
			- 与cudaDeviceSynchronize类似，但是这个是针对某一个stream的，只同步指定stream中的cpu/gpu，其他的不管。影响单个流和CPU，其他流不受影响
		- `cudaThreadSynchronize();`
			- 现在已经不推荐使用的方法
		- Synchronize using Events
			- Create specific `Events`, within streams, to use for synchronization
			- `cudaEventRecord(event, streamid)`
			- `cudaEventSynchronize(event)`
			- `cudaStreamWaitEvent(stream, event)`
			- `cudaEventQuery(event)`
			- ==使用`cudaStreamWaitEvent`同步流——这是一个非常高级的用法，可以做到极细粒度的同步，但是几乎用不到==
				- ![image.png](../assets/image_1731749064523_0.png)
				- 绿色流1中插入event1后，在紫色流2中等待流1的event执行完，才执行后续操作。
		- `cudaStreamQuery()`
			- 查询一个流任务是否完成
		- `__syncthreads();`
			- 线程块内同步
	- ## 共享内存
	  collapsed:: true
		- 我们一般在`cudaMalloc`时都是在global memory上进行访问的
		- ![image.png](../assets/image_1731458259648_0.png)
		- ![image.png](../assets/image_1731458276849_0.png)
		- `on-chip`:芯片上。`off-chip`芯片外
		- `texture memory`也有一个texture cache，在上右图底部Tex，是on-chip的，比global memory、texture memory快
		-
		- 概念：一种特殊类型的内存，其内容在源代码中被显式声明和使用
			- 位于处理器中
			- 以更高的速度访问(延迟&吞吐)
			- 仍然被内存访问指令访问
			- 在计算机体系结构中通常称为暂存存储器
		- 共享内存特点
			- 读取速度等同于缓存，在很多显卡上，缓存和共享内存使用的是同一块硬件，并且可以配置大小
			- 共享内存属于线程块block，可以被一个block内的所有线程访问
			- 共享内存的两种申请空间方式，静态申请和动态申请
			- 共享内存的大小只有几十K，过度使用共享内存会降低程序的并行性
				- 共享内存的大小有限，过度使用共享内存相当于过度频繁读写共享内存，过度频繁读写使得有些线程要排队，并行变串行，效率大幅降低
		- 使用方法
			- 申请
				- `__shared__`关键字
				- 静态申请：申请时就指定大小
					- ```cpp
					  __shared__ int s[64]; // 申请时指定大小
					  ```
				- 动态申请：申请时不指定大小，调用时才定义
					- ```cpp
					  extern __shared__ int s[];// 为了区别静态申请，需要加上exctern关键字。告诉编译器
					  // 那么需要分配的共享内存的大小就需要在调用核函数的时候明确的指出了：
					  dynamicReverse<<<1, n, n * sizeof(int)>>>; // n * sizeof(int) 的大小
					  ```
			- 使用
				- 将每个线程从全局索引位置读取元素，将它存储到共享内存之中。
				- 注意数据存在着交叉，应该将边界上的数据拷贝进来。
				- 块内线程同步：`__syncthreads()`
					- __syncthreads()将确保线程块中的每个线程都执行完`__syncthreads()` 前面的语句后，才会执行下一条语句。
						- 所以在进行线程同步的时候不可以使用 if-else 等分支语句。
							- ```cpp
							  // 下面的代码可能会导致块中的线程无限期地等待对方， 
							  // 因为块中的所有线程没有达到相同的障碍点
							  if (threadID % 2 == 0) {
							      __syncthreads();
							  } else {
							      __syncthreads();
							  }
							  ```
					- `__syncthreads`用于协调同一块中线程间的通信。 当块中的某些线程访问共享内存或全 局内存中的同一地址时，会有潜在问题（写后读、 读后写、 写后写），这将导致在那些内存位置产生未定义的应用程序行为和未定义的状态。 可以通过利用冲突访问间的同步线程 来避免这种情况。
					- 告诉其他线程我在这块进行了更改，不要出现时序上的错乱，如：读后写、写后写等
					- 试想，一个block的所有线程都会操作同一个共享内存，达到数据共享的目的，那么其中一个对其有了操作，肯定要通知其他人进行同步，不要用旧的数据，不要用错了。类似git仓库的操作了，我push了一个代码，也得通知其他人pull到本地，不要用错了
		- ==使用思路==
			- 思路上就是block中的先让所有线程一对一从global memory读取一个数据到共享内存后，在`_syncthreads`等待同步，其余线程后续读取该数据就会直接从`shared memory`读取了，不会再访问global memory，这就是速度提升的原因。
	- ## bank confilict
	  collapsed:: true
		- 概念：一个wrap中的线程访问同一个bank中不同地址的数据
		- bank是什么
			- block按32分为多个wrap，由SM调度wrap同时执行一条指令。GPU为了实现对共享内存的高效读取，将shared memory访问地址也按32进行划分。这个32可以是32个4B或8B，称为bank。其后的字节重新映射到0~31的bank。
			- 大白话：就是共享内存总线将连续内存分为32个位置，每个位置单独对应总线的部分带宽，从而享受不冲突的高速存取速度，这32个总线的划分就理解为bank，如果有多个线程同时需要访问一个bank下的不同地址，就需要串行多次访存才能取出
		- 为什么会有bank confilict？
			- [共享内存之bank冲突 - CUDA C/C++编程学习 - SegmentFault 思否](https://segmentfault.com/a/1190000007533157)
			- [CUDA编程！深入剖析静态/动态共享内存与Bank Conflict（附源码）-CSDN博客](https://blog.csdn.net/CV_Autobot/article/details/134086660)
			- [[nsight compute使用指南] 查看存储体冲突-CSDN博客](https://blog.csdn.net/fumingxiaoshen/article/details/141257748#:~:text=%E5%9C%A8Summary%E7%95%8C%E9%9D%A2%EF%BC%8C%E5%8F%8C%E5%87%BB%E5%87%BD%E6%95%B0%E5%90%8D%EF%BC%8C%E8%BF%9B%E5%85%A5Details%E7%95%8C%E9%9D%A2%20%E6%89%BE%E5%88%B0Memory%20Workload%20Analysis%E6%A0%8F%EF%BC%8C%E7%82%B9%E5%87%BB%E5%B7%A6%E8%BE%B9%E7%9A%84%E4%B8%89%E8%A7%92%E5%BD%A2%20%E5%9C%A8Memory,Chart%E7%9A%84%E4%B8%8B%E6%8B%89%E6%A1%86%E4%B8%AD%EF%BC%8C%E9%80%89%E4%B8%ADMemory%20Tables%20%E5%9C%A8Shared%20Tables%E8%A1%A8%E4%B8%8B%E7%9A%84Bank%20conflicts%E6%A0%8F%E4%B8%AD%EF%BC%8C%E5%8D%B3%E5%8F%AF%E5%BE%97%E5%88%B0%E5%AD%98%E5%82%A8%E4%BD%93%E5%86%B2%E7%AA%81%E7%9A%84%E6%95%B0%E9%87%8F%E3%80%82)
			- 数据类型导致bank confilict
				- 上述中介绍bank以4B或8B划分，如果是char、short等类型的数据，就会导致连续的数据存在同一个bank中，产生bank confilict
			- 步长导致bank confilict
				- 如32的方阵按列优先存储，那么按行访问的时候，一行的元素会映射到同一列的bank，访问时导致bank confilict
		- 如何解决bank confilict？
			- 加上pad
			- 选择合适的数据类型
			- 对矩阵行优先访问
		- 示例：
			- ```cpp
			  #define BDIMX 32
			  #define BDIMY 32
			  
			  // 1. st不冲突, ld冲突
			  __global__ void half_conflict_transfer(float *in, float *out) {
			    __shared__ float tile[BDIMY][BDIMX];
			    unsigned int idx = threadIdx.y * blockDim.x + threadIdx.x;
			    tile[threadIdx.y][threadIdx.x] = in[idx];
			    __syncthreads();
			    out[idx] = tile[threadIdx.x][threadIdx.y]; // 冲突
			  }
			  // 2. ld/st 全冲突
			  __global__ void conflict_column_transfer(float *in, float *out) {
			    __shared__ float tile[BDIMX][BDIMY];
			    unsigned int idx = threadIdx.y * blockDim.x + threadIdx.x;
			    tile[threadIdx.x][threadIdx.y] = in[idx];
			    __syncthreads();
			    out[idx] = tile[threadIdx.x][threadIdx.y];
			  }
			  //3. 无冲突访存
			  __global__ void simple_transfer(float *in, float *out) {
			    __shared__ float tile[BDIMY][BDIMX];
			    unsigned int idx = threadIdx.y * blockDim.x + threadIdx.x;
			    tile[threadIdx.y][threadIdx.x] = in[idx];
			    __syncthreads();
			    out[idx] = tile[threadIdx.y][threadIdx.x];
			  }
			  //4. 全局访存
			  __global__ void global_transfer(float *in, float *out) {
			    unsigned int idx = threadIdx.y * blockDim.x + threadIdx.x;
			    out[idx] = in[idx];
			  }
			  
			  ```
	- ## stream and event
	  collapsed:: true
		- [[1]NVIDIA: Stream and concurrency webinar](../assets/StreamsAndConcurrencyWebinar.pdf)
		- ### 什么是stream
		  collapsed:: true
			- >"A sequence of operation that execute in issue-order in GPU"
			- CUDA stream是GPU上task 的执行队列，所有CUDA操作（kernel，内存拷贝等）都是在stream上执行的。
			- 同一个流的执行顺序和各个kernel以及memcpy operation的启动顺序是一致的。但是，只要==资源没有被占用==，不同流之间的执行是可以overlap的
			- CPU和GPU的数据传输是经过PCIe总线的，PCIe是共享的，memcpy同一时间只能够执行一个
				- 我理解：硬件层面同时只能往一个方向传输数据，但是编程层面可以和GPU使用异步传输方式。
				- 带有双工PCIe总线的设备可以重叠两个数据传输，但它们必须在不同的流和不同的方向上。
			- SM计算资源是有限的，所以如果计算资源满了，单流和多流是差不多的。
			- ![image.png](../assets/image_1731737481834_0.png)
			- cuda stream有两种
				- 显示流
					- 我们自己申请创建的流
				- default stream
					- 当我们不指定核函数以及memcpy的流时，cuda会使用默认流(default stream)
					- 默认流与显式流交叉使用时注意事项：
					  background-color:: red
						- 单线程下默认流的表现：[GPU Pro 提示：CUDA 7 流简化并发 |NVIDIA 技术博客](https://developer.nvidia.com/blog/gpu-pro-tip-cuda-7-streams-simplify-concurrency/)
							- ```cpp
							  for (int i = 0; i < num_streams; i++) {
							    // launch one worker kernel per stream
							    kernel<<<1, 64, 0, streams[i]>>>(data[i], N);
							    // launch a dummy kernel on the default stream
							    kernel<<<1, 1>>>(0, 0);
							  }
							  ```
							- 单线程内，默认流的执行是同步的，显式流的执行是并行的。即：只要交叉调用了默认流和显式流，那么默认流执行前会隐式对CPU、GPU所有操作进行同步，我理解相当于调用了一次`cudaDeviceSyncronize()`。这导致交叉调用的显式流也变为了串行。
								- 解决：单线程内，编译加上`--default-stream per-thread`后，默认流的执行是异步并行的，显式流的执行是异步并行的。
							- 多线程下，默认多线程共享一个默认流 多对1，流内就会串行执行了，编译加上`--default-stream per-thread`后，每个线程独有一个默认流，互相可以并行。
							- 编译加上`--default-stream per-thread`后cuda将默认流去掉归类为显式流，从而实现并行执行。如果在代码中用到了流，务必加上这个编译参数。
			- Pageable memory 可分页内存 和 Pinned memory or page-locked memory 页锁定内存
				- 页锁定内存使用cudaMallocHost or cudaHostAlloc在CPU端申请
				- 当使用多流并行的时候，就需要使用pinned memory。
				- 大白话来解释pinned memory就是锁定在内存的页面，不会置换出磁盘。
					- 因为是CPU和GPU异步，现在开并行，那么CPU就可以去干其他事情，自然需要避免CPU上目前申请的数据被置换出去，导致后面GPU传回使用的时候需要再置换一遍进来。我目前的粗浅理解。
		- ### 多stream为什么有效？
		  collapsed:: true
			- 多流为什么会有效，流越多越好么？
				- 一、PCIe总线传输速度慢，是瓶颈，会导致传输数据的时候GPU处于空闲等待状态。
			- 多流可以实现数据传输与kernel计算的并行。
				- 二、一个kernel往往用不了整个GPU的算力。多流可以让多个kernel同时计算，充分利用GPU算力。
				- 三、不是流越多越好。GPU内可同时并行执行的流数量是有限的，因为运行的硬件资源SM有限。
			- CUDA加速常用方法：kernel合并，将小任务合并成大任务，通常比多流更有效，多流使用场景很少。
				- GPU kernel耗时最大在哪里？
					- 计算密集型：耗时在计算，一次访存，数十次甚至上百次计算
					- 访存密集型：耗时在访存，一次访存，几次计算
				- 大部分kernel都是访存密集型，就像之前shared memory优化矩阵乘法，将小任务合并为一次计算多个任务并利用shared memory，从而加速。
				- 如下例：kernel合并一般比多流更有效：
					- ![image.png](../assets/image_1731745455994_0.png)
					- 启动橙色的`A*C`起点不在直线上，是因为多流启动`A*D`的流需要一点时间，然后再往下执行启动`A*C`，所以起点不在直线上
		- ### cuda编程中的显式隐式同步
		  collapsed:: true
			- > "GPU kernels are asynchronous with host by default"
			- 显式同步
				- ((672e2f6e-602d-4a35-aac4-7edf896c0af8))
			- 隐式同步
				- These operations implicitly synchronize all other CUDA operations
					- Page-locked memory allocation
						- cudaMallocHost
						- cudaHostAlloc
					- Device memory allocation
						- cudaMalloc
					- Non-Async version of memory operations
						- cudaMemcpy*   (no Async suffix)
						- cudaMemset*    (no Async suffix)
					- Change to L1/shared memory configuration
						- cudaDeviceSetCacheConfig
		- ### 如何利用多流进行隐藏访存和核函数执行延迟的调度——代码示例
		  collapsed:: true
			- ```cpp
			  // 1. 创建流
			  cudaStream_t stream[count];
			  for (int i = 0; i < count; ++ i) {
			    CUDA_CHECK(cudaStreamCreate(&stream[i]));
			  }
			  // 2. 使用 异步传输数据,末尾指定流
			  CUDA_CHECK(cudaMemcpyAsyncP(..., stream[i]));
			  // 3. 可以使用cuda API同步
			  cudaDeviceSyncronize();
			  cudaSteamSyncronize(streamid);
			  时间同步等
			  
			  // 4. 使用完后释放流
			  for (int i = 0; i < count; ++ i) {
			  	cudaStreamDestroy(stream[i]);
			  }
			  ```
		- ### Stream Scheduling
		  collapsed:: true
			- Fermi hardware has 3 queues
				- 1 Compute Engine queue
				- 2 Copy Engine queues – one for H2D and one for D2H
				- 费米硬件有三个队列：一个计算引擎队列、两个拷贝引擎队列
			- CUDA operations are dispatched to HW(hard ware) in the sequence they were issued 流内按顺序执行
				- Placed in the relevant queue 放置在相关队列中
				- Stream dependencies between engine queues are maintained, but lost within an engine queue
			- A CUDA operation is dispatched from the engine queue if:
				- Preceding calls in the same stream have completed,
				- Preceding calls in the same queue have been dispatched, and Resources are available
			- CUDA kernels may be executed concurrently if they are in different streams
				- Threadblocks for a given kernel are scheduled if all threadblocks for preceding kernels have been scheduled and there still are SM resources available
			- Note a blocked operation blocks all other operations in the queue, even in other streams
		- ### stream API
		  collapsed:: true
			- 定义
				- cudaStream_t stream;
			- 创建
				- cudaStreamCreate(&stream);
			- 数据传输
				- cudaMemcpyAsync(dst, src, size, type, stream)
			- kernel在流中执行
				- kernel_name<<>>(argument list);
			- 同步和查询
				- cudaError_t cudaStreamSynchronize(cudaStream_t stream)
				- cudaError_t cudaStreamQuery(cudaStream_t stream);
			- 销毁
				- cudaError_t cudaStreamDestroy(cudaStream_t stream);
			- GPU 算力3.5 及以上，即Kepler架构及以上 API：可以给stream设置优先级，优先级高的优先调度执行
				- `cudaError_t cudaStreamCreateWithPriority(cudaStream_t* pStream, unsigned int flags, int priority);`
				- `cudaError_t cudaDeviceGetStreamPriorityRange(int *leastPriority, int *greatestPriority);`
				- Tips：
					- 优先级只对kernel有效，对内存拷贝无效
					- 较低的整数值表示较高的流优先级。
		- ### cuda event API
			- CUDA Event，在stream中插入一个事件，类似于打一个标记位，用来记录stream是否执行到当前位置。Event有两个状态，已被执行和未被执行。
			- 定义
				- `cudaEvent_t event`
			- 创建
				- `cudaError_t cudaEventCreate(cudaEvent_t* event);`
			- 插入流中
				- `cudaError_t cudaEventRecord(cudaEvent_t event, cudaStream_t stream = 0);`
				- 默认加入默认流中
			- 销毁
				- `cudaError_t cudaEventDestroy(cudaEvent_t event);`
			- 同步和查询
				- `cudaError_t cudaEventSynchronize(cudaEvent_t event);`
					- 等待直到event执行
				- `cudaError_t cudaEventQuery(cudaEvent_t event);`
					- 查询event是否被执行
			- 进阶同步函数
				- `cudaError_t cudaStreamWaitEvent(cudaStream_t stream, cudaEvent_t event);`
					- 该函数会指定stream等待特定的event，该event可以关联到相同或者不同的stream
			- 常用来测耗时
		-
- # cuda ERROR Handle #cuda
  collapsed:: true
	-
- # Nsight system and compute——cuda性能分析
  collapsed:: true
	- ## 安装
		- 直接搜索`Nsight system download`和`Nsight compute download`即可
			- [Nsight Systems - Get Started | NVIDIA Developer](https://developer.nvidia.com/nsight-systems/get-started)
			- [Getting Started with Nsight Compute | NVIDIA Developer](https://developer.nvidia.com/tools-overview/nsight-compute/get-started)
		-
- # cudnn
  collapsed:: true
	- ## 调用流程
		- 创建cuDNN句柄
			- cudnnStatus_t cudnnCreate(cudnnHandle_t *handle)
		- 以Host方式调用在Device上运行的函数
			- 比如卷积运算：cudnnConvolutionForward等
		- 释放cuDNN句柄
			- cudnnStatus_t cudnnDestroy(cudnnHandle_t handle)
		- 将CUDA流设置&返回成cudnn句柄
			- cudnnStatus_t cudnnSetStream( cudnnHandle_t handle, cudaStream_t streamId)
			- cudnnStatus_t cudnnGetStream( cudnnHandle_t handle, cudaStream_t *streamId)
	- ## 常用函数接口
		- 可以搜索文档：[cudnn](https://docs.nvidia.com/deeplearning/cudnn/)
		- cudnnConvolutionForward
			- ```cpp
			  cudnnStatus_t cudnnConvolutionForward(
			  	cudnnHandle_t					   handle,   句柄
			  	const void                         *alpha,   卷积相乘的权重
			      const cudnnTensorDescriptor_t  	   xDesc,	 张量的描述，维度、大小等
			   	const void                         *x,       张量的数据地址
			  	const cudnnFilterDescriptor_t 	   wDesc,    卷积核的描述
			   	const void                         *w,       没明白
			   	const cudnnConvolutionDescriptor_t convDesc, 卷积的描述
			   	cudnnConvolutionFwdAlgo_t          algo,     卷积的算法
			   	void                               *workSpace,显存地址
			  	size_t 							   workSpaceSizeInBytes,显存大小
			   	const void                         *beta,    原来的张量保留的权重，不保留则为0
			  	const cudnnTensorDescriptor_t 	   Desc,     输出结果张量的描述和地址
			   	void                           	   *y)
			  ```
		- cudnnSetFilter4dDescriptor
			- ```cpp
			  cudnnStatus_t cudnnSetFilter4dDescriptor(
			      cudnnFilterDescriptor_t    filterDesc,
			      cudnnDataType_t            dataType,
			      cudnnTensorFormat_t        format,
			      int                        k,
			      int                        c,
			      int                        h,
			      int                        w)
			  ```
		- Scaling factors alpha and beta can be used to scale the input tensor and the output tensor respectively.
- # tensorrt
  collapsed:: true
	- 不同架构生成的engine不能兼容
		- 因为在一个硬件架构中生成的engine会根据硬件特性进行调优，切换到不一样的硬件架构自然就用不了了。
	- ## 基本使用流程
		- ![image.png](../assets/image_1732460618571_0.png)
		- ### TRT转换优化引擎->生成engine
			- 创建Builder
			- 创建Network
			- 使用API or Parser构建network
			- 优化和转换网络
			- 序列化和反序列化模型
		- ### TRT执行引擎
			- 传输计算数据(host -> device)
			- 执行计算
			- 传输计算结果(device -> host)
		- ### 代码示例
			- ![image.png](../assets/image_1732460829573_0.png)
			- ILogger是用于输出log的。
		- ### demo SampleMNIST
		-