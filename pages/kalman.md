- > Where do the names `z`, `P`, `Q`, and `R` come from? You will see them used in the rest of this book. In the literature $R$ is nearly universally used for the measurement noise, $Q$ for the process noise and $P$ for the variance of the state. Using $z$ for the measurement is common, albeit not universal. Almost every book and paper you read will use these variable names. Get used to them.
- `z`, `P`, `Q`, and `R`åˆ†åˆ«ä»£è¡¨ä»€ä¹ˆæ„æ€ï¼Ÿ
	- > è¿™å‡ ä¸ªå€¼éƒ½åªæ˜¯çœ‹ç€å“äººï¼Œå­¦æœ¯æ–‡ç« ä¸­çš„åžƒåœ¾å‘½åã€‚å…¶å®žéƒ½æ˜¯å¾ˆå®¹æ˜“ç†è§£çš„ä¸œè¥¿
	- $R$æ˜¯æµ‹é‡å€¼çš„å™ªå£°(æ–¹å·®)ã€‚
	- $Q$æ˜¯è¿‡ç¨‹å™ªå£°ï¼Œ
	- $P$æ˜¯çŠ¶æ€æ–¹å·®ï¼Œ
	- $z$æ˜¯æµ‹é‡å€¼ã€‚
	- $K$æ˜¯å¡å°”æ›¼å¢žç›Š
		- æœ¬è´¨å°±æ˜¯ä¸€ä¸ªæµ‹é‡å€¼å’Œé¢„æµ‹å€¼ä¹‹é—´çš„ä¸€ä¸ªåå‘çš„æ¦‚çŽ‡è€Œå·²
			- ((6786923d-3f6c-44fd-b249-2a00f4906428))
- ## ä¸€ç»´å¡å°”æ›¼æ»¤æ³¢ #kalman
  collapsed:: true
	- é—®é¢˜æè¿°ï¼š
		- ä¸€åªç‹—åœ¨é©¬æ‹‰æ¾ä¸­ç§»åŠ¨ã€‚è„–å­ä¸Šçš„é¡¹åœˆæœ‰ä¸€ä¸ªä¼ æ„Ÿå™¨ï¼Œä¼ æ„Ÿå™¨çš„å€¼æœ‰è¯¯å·®ï¼Œä½†æ˜¯è¯¯å·®ä¸€èˆ¬å¾ˆå°ï¼Œå¦‚ï¼šç‹—ç§»åŠ¨äº†23mï¼Œä¼ æ„Ÿå™¨å¯èƒ½ä¼šç»™å‡º22.9mæˆ–è€…23.1mï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨é«˜æ–¯å¯¹å…¶å»ºæ¨¡ã€‚æˆ‘ä»¬é¢„æµ‹ç‹—çš„ç§»åŠ¨ï¼Œå¯èƒ½è¿‡äº†å¤´ï¼Œä¹Ÿå¯èƒ½æ²¡è¿‡å¤´ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨é«˜æ–¯æ¥å»ºæ¨¡ã€‚
	- æˆ‘ä»¬å¯ä»¥ç”¨é«˜æ–¯æ¥æè¿°å¯¹ç‹—çš„ä½ç½®çš„ç½®ä¿¡åº¦ã€‚åˆå§‹æˆ‘ä»¬ç›¸ä¿¡ç‹—åœ¨10mçš„ä½ç½®ï¼Œæ–¹å·®ä¸º1.
		- ```python
		  # 1. å‡è®¾ç‹—çš„ä½ç½®çš„ä¿¡å¿µç¬¦åˆæ­£æ€åˆ†å¸ƒ
		  from scipy.stats import norm
		  import numpy as np
		  import matplotlib.pyplot as plt
		  
		  
		  gaussion = norm(10, 1)
		  xs = np.arange(4, 16, 0.05)
		  ys = [gaussion.pdf(x) for x in xs]
		  
		  plt.plot(xs, ys)
		  plt.xlim(4, 16)
		  plt.ylim(0, 0.5)
		  plt.show()
		  ```
		- ![one-dimensional-kalman.png](../assets/one-dimensional-kalman_1722958705620_0.png)
		- ä»Žå›¾ä¸­å¯ä»¥çœ‹å‡ºæˆ‘ä»¬å¯¹ç‹—çš„ä½ç½®å¹¶ä¸ç¡®å®šï¼Œåªèƒ½è®¤ä¸ºç‹—æžæœ‰å¯èƒ½åœ¨10mçš„ä½ç½®ï¼Œä½†9~11mçš„ä»»ä½•ä½ç½®éƒ½æœ‰å¯èƒ½ã€‚å‡è®¾æˆ‘ä»¬è¯»å–ä¼ æ„Ÿå™¨500æ¬¡ï¼Œæ¯æ¬¡è¿”å›žçš„å€¼éƒ½åœ¨8~12ä¹‹é—´ï¼Œä»¥10ä¸ºä¸­å¿ƒï¼Œæˆ‘ä»¬åº”è¯¥éžå¸¸ç¡®ä¿¡ç‹—åœ¨10é™„è¿‘ã€‚
			- ```python
			  # 2. å‡è®¾è¯»å–ä¼ æ„Ÿå™¨500æ¬¡,æ‰€æœ‰æ•°å€¼éƒ½åœ¨10é™„è¿‘:
			  import numpy as np
			  from numpy.random import randn
			  import matplotlib.pyplot as plt
			  
			  # range(500) ç”Ÿæˆä¸€ä¸ªèŒƒå›´å¯¹è±¡ï¼Œè¿™ä¸ªå¯¹è±¡è¡¨ç¤ºä»Ž 0 å¼€å§‹åˆ° 499 ç»“æŸçš„æ•´æ•°åºåˆ—
			  xs = range(500) 
			  ys = randn(500)*1. + 10.
			  plt.plot(xs, ys)
			  plt.show()
			  print(f'Mean of readings is {np.mean(ys):.3f}')
			  # Mean of readings is 10.004
			  ```
				- **`randn(500)`**
					- ç”Ÿæˆä¸€ä¸ªåŒ…å«Â **500ä¸ªæ ·æœ¬**Â çš„æ•°ç»„ï¼Œè¿™äº›æ ·æœ¬æœä»ŽÂ **æ ‡å‡†æ­£æ€åˆ†å¸ƒ**ï¼ˆå‡å€¼ä¸ºÂ `0`ï¼Œæ ‡å‡†å·®ä¸ºÂ `1`ï¼‰ã€‚
				- **`* 1.`**
					- å°†æ¯ä¸ªæ ·æœ¬ä¹˜ä»¥Â `1.0`ï¼ˆå³ä¿æŒæ•°å€¼ä¸å˜ï¼‰ã€‚è¿™ä¸€æ­¥é€šå¸¸ç”¨äºŽè°ƒæ•´æ ‡å‡†å·®ï¼Œä¾‹å¦‚ä¹˜ä»¥Â `2.0`Â ä¼šå°†æ ‡å‡†å·®å˜ä¸ºÂ `2`ã€‚æ­¤å¤„æ— å®žé™…ä½œç”¨ï¼Œå¯èƒ½æ˜¯ä¸ºäº†ä»£ç å¯æ‰©å±•æ€§ã€‚
				- **`+ 10.`**
					- å°†æ¯ä¸ªæ ·æœ¬åŠ ä¸ŠÂ `10.0`ï¼Œè¿™ä¼šå°†æ•´ä¸ªåˆ†å¸ƒçš„å‡å€¼ä»ŽÂ `0`Â åç§»åˆ°Â `10`ã€‚
			- ![range-500.png](../assets/range-500_1722959431451_0.png)
			- å‡è®¾ç‹—ä¸€ç›´é™æ­¢ï¼Œæˆ‘ä»¬å°±è¯´ç‹—åœ¨10çš„ä½ç½®ï¼Œæ–¹å·®ä¸º1.
	- ä½¿ç”¨é«˜æ–¯æ¦‚çŽ‡è¿›è¡Œè¿½è¸ª
		- trackingæ˜¯é€šè¿‡é¢„æµ‹å’Œæ›´æ–°çš„å¾ªçŽ¯è¿›è¡Œçš„ï¼Œæˆ‘ä»¬ä½¿ç”¨å¦‚ä¸‹æ–¹ç¨‹è®¡ç®—ï¼š
			- $$\begin{aligned} 
			  \bar {\mathbf x} &= \mathbf x \ast f_{\mathbf x}(\bullet)\, \, &\text{Predict} \\
			  \mathbf x &= \mathcal L \cdot \bar{\mathbf x}\, \, &\text{Update}
			  \end{aligned}$$
			- $\bar{\mathbf x}$æ˜¯`prior`ï¼Œ$\mathcal L$æ˜¯ç»™å®š$\bar{\mathbf x}$çš„`likelihood`ï¼Œ$f_{\mathbf x}(\bullet)$æ˜¯`process model`ï¼Œ$\ast$è¡¨ç¤ºå·ç§¯`convolution`ï¼Œ$\mathbf x$ç²—ä½“è¡¨ç¤ºå®ƒæ˜¯æ•°å­—çš„ç›´æ–¹å›¾æˆ–çŸ¢é‡ã€‚
		- è¿™ç§æ–¹æ³•ä¼šäº§ç”Ÿç›´æ–¹å›¾ï¼Œè¿™æ„å‘³ç€ç‹—å¯èƒ½åŒæ—¶å‡ºçŽ°åœ¨å¤šä¸ªåœ°æ–¹ã€‚æ­¤å¤–ï¼Œå¯¹äºŽå¤§åž‹é—®é¢˜ï¼Œè®¡ç®—é€Ÿåº¦ä¼šå¾ˆæ…¢ã€‚
		- æˆ‘ä»¬å¯ä»¥ä½¿ç”¨é«˜æ–¯$\mathcal N(x, \sigma^2)$æ¥æ›¿ä»£$\mathbf x$å’Œç›´æ–¹å›¾
			- ![image.png](../assets/image_1722960173363_0.png)
			- é«˜æ–¯åˆ†å¸ƒçš„å°¾éƒ¨åœ¨ä¸¤è¾¹å»¶ä¼¸åˆ°æ— ç©·å¤§ï¼Œæ‰€ä»¥å®ƒåœ¨ç›´æ–¹å›¾ä¸­åŒ…å«ä»»æ„å¤šçš„æ¡å½¢å›¾ã€‚å¦‚æžœè¿™ä»£è¡¨äº†æˆ‘ä»¬å¯¹ç‹—åœ¨èµ°å»Šä¸Šçš„ä½ç½®çš„ä¿¡å¿µï¼Œé‚£ä¹ˆè¿™ä¸ªé«˜æ–¯åˆ†å¸ƒè¦†ç›–äº†æ•´ä¸ªèµ°å»Š(ä»¥åŠæ•´ä¸ªå®‡å®™åœ¨è¿™ä¸ªè½´ä¸Š)ã€‚æˆ‘ä»¬è®¤ä¸ºç‹—å¾ˆå¯èƒ½åœ¨10ï¼Œä½†å®ƒå¯èƒ½åœ¨8ã€14ï¼Œæˆ–è€…åœ¨æžå°çš„æ¦‚çŽ‡ä¸‹ï¼Œåœ¨10$^{80}$ã€‚
			- ç”¨é«˜æ–¯æ›¿æ¢ç›´æ–¹å›¾ï¼š
				- $$\begin{array}{l|l|c}
				  \text{discrete Bayes} & \text{Gaussian} & \text{Step}\\
				  \hline
				  \bar {\mathbf x} = \mathbf x \ast f(\mathbf x) & 
				  \bar {x}_\mathcal{N} =  x_\mathcal{N} \, \oplus \, f_{x_\mathcal{N}}(\bullet) &
				  \text{Predict} \\
				  \mathbf x = \|\mathcal L \bar{\mathbf x}\| & x_\mathcal{N} = L \, \otimes \, \bar{x}_\mathcal{N} & \text{Update} 
				  \end{array}$$
				- å…¶ä¸­$\oplus$å’Œ$\otimes$è¡¨ç¤ºé«˜æ–¯å‡½æ•°ä¸Šçš„æœªçŸ¥ç®—å­ã€‚ä¸‹æ ‡è¡¨ç¤º$x_\mathcal{N}$æ˜¯é«˜æ–¯åˆ†å¸ƒã€‚
			- ç¦»æ•£è´å¶æ–¯æ»¤æ³¢å™¨ä½¿ç”¨å·ç§¯è¿›è¡Œé¢„æµ‹ã€‚æˆ‘ä»¬è¯æ˜Žäº†å®ƒä½¿ç”¨äº†æ€»æ¦‚çŽ‡å®šç†ï¼Œä»¥æ±‚å’Œçš„å½¢å¼è®¡ç®—ï¼Œæ‰€ä»¥ä¹Ÿè®¸æˆ‘ä»¬å¯ä»¥åŠ ä¸Šé«˜æ–¯å‡½æ•°ã€‚å®ƒä½¿ç”¨ä¹˜æ³•å°†æµ‹é‡å€¼åˆå¹¶åˆ°å…ˆéªŒä¸­ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥å°†é«˜æ–¯å‡½æ•°ç›¸ä¹˜ã€‚
			- $$\begin{aligned} 
			  \bar x &\stackrel{?}{=} x + f_x(\bullet) \\
			  x &\stackrel{?}{=} \mathcal L \cdot \bar x
			  \end{aligned}$$
			- > This will only work if the sum and product of two Gaussians is another Gaussian. Otherwise after the first epoch $x$ would not be Gaussian, and this scheme falls apart.
	- ä½¿ç”¨é«˜æ–¯é¢„æµ‹
	- ä½¿ç”¨é«˜æ–¯æ›´æ–°
	- Last Source Code:
		- ```python
		  # 1. å‡è®¾ç‹—çš„ä½ç½®çš„ä¿¡å¿µç¬¦åˆæ­£æ€åˆ†å¸ƒ
		  from scipy.stats import norm
		  import numpy as np
		  import matplotlib.pyplot as plt
		  
		  
		  gaussion = norm(10, 1)
		  xs = np.arange(4, 16, 0.05)
		  ys = [gaussion.pdf(x) for x in xs]
		  
		  plt.plot(xs, ys)
		  plt.xlim(4, 16)
		  plt.ylim(0, 0.5)
		  plt.show()
		  
		  # 2. å‡è®¾è¯»å–ä¼ æ„Ÿå™¨500æ¬¡,æ‰€æœ‰æ•°å€¼éƒ½åœ¨10é™„è¿‘:
		  import numpy as np
		  from numpy.random import randn
		  import matplotlib.pyplot as plt
		  
		  # range(500) ç”Ÿæˆä¸€ä¸ªèŒƒå›´å¯¹è±¡ï¼Œè¿™ä¸ªå¯¹è±¡è¡¨ç¤ºä»Ž 0 å¼€å§‹åˆ° 499 ç»“æŸçš„æ•´æ•°åºåˆ—
		  xs = range(500) 
		  ys = randn(500)*1. + 10.
		  plt.plot(xs, ys)
		  plt.show()
		  print(f'Mean of readings is {np.mean(ys):.3f}')
		  
		  # æä¾›__repr__å‡½æ•°ï¼Œä½¿ç”¨å…ƒç»„è¡¨ç¤ºé«˜æ–¯å‡½æ•°ï¼Œ g[0]è¡¨ç¤ºå‡å€¼,g[1]è¡¨ç¤ºæ–¹å·®
		  from collections import namedtuple
		  gaussian = namedtuple('Gaussian', ['mean', 'var'])
		  gaussian.__repr__ = lambda s: f'ð’©(Î¼={s[0]:.3f}, ðœŽÂ²={s[1]:.3f})'
		  # ä»Žè€Œå¯ä»¥ä½¿ç”¨g1 = gaussian(3.4, 10.1)
		  g1 = gaussian(3.4, 10.1)
		  g2 = gaussian(mean=4.5, var=0.2**2)
		  print(g1)
		  print(g2)
		  # å¯ä»¥ä½¿ç”¨ä»¥ä¸‹æ–¹æ³•è®¿é—®å‡å€¼å’Œæ–¹å·®
		  g1.mean, g1[0], g1[1], g1.var
		  
		  
		  
		  # ä»¥ä¸‹æ˜¯é«˜æ–¯å‡½æ•°çš„predictå‡½æ•°å®žçŽ°,å…¶ä¸­poså’Œmovementæ˜¯é«˜æ–¯å…ƒç»„ï¼Œæ ¼å¼ä¸º(mean,var):
		  def predict(pos, movement):
		      return gaussian(pos.mean + movement.mean, pos.var + movement.var)
		  pos = gaussian(10., .2**2)
		  move = gaussian(15., .7**2)
		  predict(pos, move)
		  
		  # updateå®žçŽ°
		  def gaussian_multiply(g1, g2):
		      mean = (g1.var * g2.mean + g2.var * g1.mean) / (g1.var + g2.var)
		      variance = (g1.var * g2.var) / (g1.var + g2.var)
		      return gaussian(mean, variance)
		  
		  def update(prior, likelihood):
		      posterior = gaussian_multiply(likelihood, prior)
		      return posterior
		  
		  # test the update function
		  predicted_pos = gaussian(10., .2**2)
		  measured_pos = gaussian(11., .1**2)
		  estimated_pos = update(predicted_pos, measured_pos)
		  estimated_pos
		  
		  
		  ```
-
- ## Kalman Gain å¡å°”æ›¼å¢žç›Š #kalman
	- > We see that the filter works. Now let's go back to the math to understand what is happening. The posterior $x$ is computed as the likelihood times the prior ($\mathcal L \bar x$), where both are Gaussians.
	- çŽ°åœ¨ä»Žæ•°å­¦è§’åº¦ç†è§£å‘ç”Ÿäº†ä»€ä¹ˆï¼ŒåŽéªŒxæ˜¯ä¼¼ç„¶ä¹˜ä»¥å…ˆéªŒï¼Œå¹¶ä¸”éƒ½æ˜¯é«˜æ–¯å‡½æ•°
	- > Therefore the mean of the posterior is given by:
	- $$
	  \mu=\frac{\bar\sigma^2\, \mu_z + \sigma_z^2 \, \bar\mu} {\bar\sigma^2 + \sigma_z^2}
	  $$
	- I use the subscript $z$ to denote the measurement. We can rewrite this as:
	- $$\mu = \left( \frac{\bar\sigma^2}{\bar\sigma^2 + \sigma_z^2}\right) \mu_z + \left(\frac{\sigma_z^2}{\bar\sigma^2 + \sigma_z^2}\right)\bar\mu$$
	- In this form it is easy to see that we are scaling the measurement and the prior by weights:
	- $$\mu = W_1 \mu_z + W_2 \bar\mu$$
	- The weights sum to one because the denominator is a normalization term. We introduce a new term,$K=W_1$, giving us:
	- $$\begin{aligned}
	  \mu &= K \mu_z + (1-K) \bar\mu\\
	  &= \bar\mu + K(\mu_z - \bar\mu)
	  \end{aligned}$$
	- where
	- $$K = \frac {\bar\sigma^2}{\bar\sigma^2 + \sigma_z^2}$$
	- $K$ is the *Kalman gain*. It's the crux of the Kalman filter. It is a scaling term that chooses a value partway between $\mu_z$ and $\bar\mu$.
	- Let's work a few examples. If the measurement is nine times more accurate than the prior, then $\bar\sigma^2 = 9\sigma_z^2$, and
	- $$\begin{aligned}
	  \mu&=\frac{9 \sigma_z^2 \mu_z + \sigma_z^2\, \bar\mu} {9 \sigma_z^2 + \sigma_\mathtt{z}^2} \\
	  &= \left(\frac{9}{10}\right) \mu_z + \left(\frac{1}{10}\right) \bar\mu
	  \end{aligned}
	  $$
	- Hence $K = \frac 9 {10}$, and to form the posterior we take nine tenths of the measurement and one tenth of the prior.
	- If the measurement and prior are equally accurate, then $\bar\sigma^2 = \sigma_z^2$ and
	- $$\begin{gathered}
	  \mu=\frac{\sigma_z^2\,  (\bar\mu + \mu_z)}{2\sigma_\mathtt{z}^2} \\
	  = \left(\frac{1}{2}\right)\bar\mu + \left(\frac{1}{2}\right)\mu_z
	  \end{gathered}$$
	- which is the average of the two means. It makes intuitive sense to take the average of two equally accurate values.
	- We can also express the variance in terms of the Kalman gain:
	- $$\begin{aligned}
	  \sigma^2 &= \frac{\bar\sigma^2 \sigma_z^2 } {\bar\sigma^2 + \sigma_z^2} \\
	  &= K\sigma_z^2 \\
	  &= (1-K)\bar\sigma^2 
	  \end{aligned}$$
	- We can understand this by looking at this chart:
	- ![image.png](../assets/image_1723449183020_0.png)
	  id:: 6786923d-3f6c-44fd-b249-2a00f4906428
	- The Kalman gain $K$ is a scale factor that chooses a value along the residual. This leads to an alternative but equivalent implementation for `update()` and `predict()`:
		- ```python
		  def update(prior, measurement):
		      x, P = prior        # mean and variance of prior
		      z, R = measurement  # mean and variance of measurement
		      
		      y = z - x        # residual
		      K = P / (P + R)  # Kalman gain
		  
		      x = x + K*y      # posterior
		      P = (1 - K) * P  # posterior variance
		      return gaussian(x, P)
		  
		  def predict(posterior, movement):
		      x, P = posterior # mean and variance of posterior
		      dx, Q = movement # mean and variance of movement
		      x = x + dx
		      P = P + Q
		      return gaussian(x, P)
		  ```
	-
-